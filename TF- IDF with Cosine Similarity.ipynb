{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "1. Term Frequency (TF)\n",
    "2. Inverse Document Frequency (IDF)\n",
    "3. TF * IDF\n",
    "4. Vector Space Models and Representation â€“ Cosine Similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us imagine that you are doing a search on below documents with the following query: **life learning**\n",
    "\n",
    "* **Document 1** : I want to start learning to charge something in life.\n",
    "* **Document 2** : learning something about me no one else knows\n",
    "* **Document 3** : Never stop learning\n",
    "\n",
    "The query is a free text query. It means a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators.\n",
    "\n",
    "Let us go over each step in detail to see how it all works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Term Frequency(TF)\n",
    "Term Frequency also known as TF measures the number of times a term (word) occurs in a document. Given below is the code and the terms and their frequency on each of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents\n",
    "doc1 = \"I want to start learning to charge something in life\"\n",
    "doc2 = \"reading something about life no one else knows\"\n",
    "doc3 = \"Never stop learning\"\n",
    "#query string\n",
    "query = \"life learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE :** Text Preprocessing Steps are ignored as the objective of this kernel is to explain and develop TF-IDF and cosine similarity from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document  life  charge  something  learning  start  in  to  want  I\n",
      "0  Term Frequency     1       1          1         1      1   1   2     1  1\n",
      "         Document  life  else  something  no  one  knows  about  reading\n",
      "0  Term Frequency     1     1          1   1    1      1      1        1\n",
      "         Document  stop  Never  learning\n",
      "0  Term Frequency     1      1         1\n"
     ]
    }
   ],
   "source": [
    "#term -frequenvy :word occurences in a document\n",
    "def compute_tf(docs_list):\n",
    "    for doc in docs_list:\n",
    "        doc1_lst = doc.split(\" \")\n",
    "        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n",
    "\n",
    "        for token in doc1_lst:\n",
    "            wordDict_1[token] +=  1\n",
    "        df = pd.DataFrame([wordDict_1])\n",
    "        idx = 0\n",
    "        new_col = [\"Term Frequency\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "        \n",
    "compute_tf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In reality each document will be of different size. On a large document the frequency of the terms will be much higher than the smaller ones. Hence we need to normalize the document based on its size. \n",
    "* A simple trick is to divide the term frequency by the total number of terms. \n",
    "* For example in Document 1 the term game occurs two times. The total number of terms in the document is 10. Hence the normalized term frequency is 2 / 10 = 0.2. \n",
    "\n",
    "\n",
    "Given below are the normalized term frequency for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Document  life  charge  something  learning  start   in   to  want  \\\n",
      "0  Normalized TF   0.1     0.1        0.1       0.1    0.1  0.1  0.2   0.1   \n",
      "\n",
      "     I  \n",
      "0  0.1  \n",
      "        Document   life   else  something     no    one  knows  about  reading\n",
      "0  Normalized TF  0.125  0.125      0.125  0.125  0.125  0.125  0.125    0.125\n",
      "        Document      stop     Never  learning\n",
      "0  Normalized TF  0.333333  0.333333  0.333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'life': 0.1,\n",
       "  'charge': 0.1,\n",
       "  'something': 0.1,\n",
       "  'learning': 0.1,\n",
       "  'start': 0.1,\n",
       "  'in': 0.1,\n",
       "  'to': 0.2,\n",
       "  'want': 0.1,\n",
       "  'I': 0.1},\n",
       " {'life': 0.125,\n",
       "  'else': 0.125,\n",
       "  'something': 0.125,\n",
       "  'no': 0.125,\n",
       "  'one': 0.125,\n",
       "  'knows': 0.125,\n",
       "  'about': 0.125,\n",
       "  'reading': 0.125},\n",
       " {'stop': 0.3333333333333333,\n",
       "  'Never': 0.3333333333333333,\n",
       "  'learning': 0.3333333333333333}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalized Term Frequency\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def compute_normalizedtf(documents):\n",
    "    tf_doc = []\n",
    "    for txt in documents:\n",
    "        sentence = txt.split()\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt)\n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"Normalized TF\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "    return tf_doc\n",
    "\n",
    "tf_doc = compute_normalizedtf([doc1, doc2, doc3])\n",
    "tf_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inverse Document Frequency (IDF)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2.09861228866811,\n",
       " 'want': 2.09861228866811,\n",
       " 'to': 2.09861228866811,\n",
       " 'start': 2.09861228866811,\n",
       " 'learning': 1.4054651081081644,\n",
       " 'charge': 2.09861228866811,\n",
       " 'something': 1.4054651081081644,\n",
       " 'in': 2.09861228866811,\n",
       " 'life': 1.4054651081081644,\n",
       " 'reading': 2.09861228866811,\n",
       " 'about': 2.09861228866811,\n",
       " 'no': 2.09861228866811,\n",
       " 'one': 2.09861228866811,\n",
       " 'else': 2.09861228866811,\n",
       " 'knows': 2.09861228866811,\n",
       " 'Never': 2.09861228866811,\n",
       " 'stop': 2.09861228866811}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverseDocumentFrequency(term, allDocuments):\n",
    "    numDocumentsWithThisTerm = 0\n",
    "    for doc in range (0, len(allDocuments)):\n",
    "        if term.lower() in allDocuments[doc].lower().split():\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0:\n",
    "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def compute_idf(documents):\n",
    "    idf_dict = {}\n",
    "    for doc in documents:\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
    "    return idf_dict\n",
    "idf_dict = compute_idf([doc1, doc2, doc3])\n",
    "\n",
    "compute_idf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we are trying to find out relevant documents for the query: **life learning**\n",
    "\n",
    "* For each term in the query multiply its normalized term frequency with its IDF on each document. \n",
    "* In Document1 for the term life the normalized term frequency is 0.1 and its IDF is 1.405465108. \n",
    "* Multiplying them together we get 0.140550715 (0.1 * 1.405465108). \n",
    "* \n",
    "Given below is TF * IDF calculations for life and learning in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc      life  learning\n",
      "0    0  0.140547  0.140547\n",
      "1    1  0.175683  0.000000\n",
      "2    2  0.000000  0.468488\n"
     ]
    }
   ],
   "source": [
    "# tf-idf score across all docs for the query string(\"life learning\")\n",
    "def compute_tfidf_with_alldocs(documents , query):\n",
    "    tf_idf = []\n",
    "    index = 0\n",
    "    query_tokens = query.split()\n",
    "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
    "    for doc in documents:\n",
    "        df['doc'] = np.arange(0 , len(documents))\n",
    "        doc_num = tf_doc[index]\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            for text in query_tokens:\n",
    "                if(text == word):\n",
    "                    idx = sentence.index(word)\n",
    "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
    "                    tf_idf.append(tf_idf_score)\n",
    "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
    "        index += 1\n",
    "    df.fillna(0 , axis=1, inplace=True)\n",
    "    return tf_idf , df\n",
    "            \n",
    "documents = [doc1, doc2, doc3]\n",
    "tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Vector Space Models and Representation  â€“ Cosine Similarity\n",
    "\n",
    "The set of documents in a collection then is viewed as a set of vectors in a vector space. Each term will have its own axis. Using the formula given below we can find out the similarity between any two documents.\n",
    "\n",
    "* > Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
    "* > Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * â€¦ * d1[n] * d2[n]\n",
    "* > ||d1|| = square root(d1[0]2 + d1[1]2 + ... + d1[n]2)\n",
    "* > ||d2|| = square root(d2[0]2 + d2[1]2 + ... + d2[n]2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vectors deals only with numbers. In this example we are dealing with text documents. This was the reason why we used TF and IDF to convert text into numbers so that it can be represented by a vecto\n",
    "\n",
    "\n",
    "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.5, 'learning': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Normalized TF for the query string(\"life learning\")\n",
    "def compute_query_tf(query):\n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query)\n",
    "    return query_norm_tf\n",
    "query_norm_tf = compute_query_tf(query)\n",
    "print(query_norm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 1.4054651081081644, 'learning': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "#idf score for the query string(\"life learning\")\n",
    "def compute_query_idf(query):\n",
    "    idf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    documents = [doc1, doc2, doc3]\n",
    "    for word in sentence:\n",
    "        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n",
    "    return idf_dict_qry\n",
    "idf_dict_qry = compute_query_idf(query)\n",
    "print(idf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.7027325540540822, 'learning': 0.7027325540540822}\n"
     ]
    }
   ],
   "source": [
    "#tf-idf score for the query string(\"life learning\")\n",
    "def compute_query_tfidf(query):\n",
    "    tfidf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    for word in sentence:\n",
    "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
    "    return tfidf_dict_qry\n",
    "tfidf_dict_qry = compute_query_tfidf(query)\n",
    "print(tfidf_dict_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now calculate the cosine similarity of the query and Document1.\n",
    "\n",
    "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "Dot product(Query, Document1) \n",
    "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
    "     = 0.197545035151\n",
    "\n",
    "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
    "\n",
    "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
    "\n",
    "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
    "                                        = 0.197545035151 / 0.197545035151\n",
    "                                        = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "\"\"\"\n",
    "Example : Dot roduct(Query, Document1) \n",
    "\n",
    "     life:\n",
    "     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n",
    "     sqrt(tfidf(life w.r.t query)) * \n",
    "     sqrt(tfidf(life w.r.t doc1))\n",
    "     \n",
    "     learning:\n",
    "     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n",
    "     sqrt(tfidf(learning w.r.t query)) * \n",
    "     sqrt(tfidf(learning w.r.t doc1))\n",
    "\n",
    "\"\"\"\n",
    "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n",
    "    dot_product = 0\n",
    "    qry_mod = 0\n",
    "    doc_mod = 0\n",
    "    tokens = query.split()\n",
    "   \n",
    "    for keyword in tokens:\n",
    "        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n",
    "        #||Query||\n",
    "        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n",
    "        #||Document||\n",
    "        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n",
    "    qry_mod = np.sqrt(qry_mod)\n",
    "    doc_mod = np.sqrt(doc_mod)\n",
    "    #implement formula\n",
    "    denominator = qry_mod * doc_mod\n",
    "    cos_sim = dot_product/denominator\n",
    "     \n",
    "    return cos_sim\n",
    "\n",
    "from collections import Iterable\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                yield x\n",
    "        else:        \n",
    "             yield item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Document1', 'Document2', 'Document3']\n",
      "[1.0, 0.7071067811865475, 0.7071067811865475]\n"
     ]
    }
   ],
   "source": [
    "def rank_similarity_docs(data):\n",
    "    cos_sim =[]\n",
    "    for doc_num in range(0 , len(data)):\n",
    "        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n",
    "    return cos_sim\n",
    "similarity_docs = rank_similarity_docs(documents)\n",
    "doc_names = [\"Document1\", \"Document2\", \"Document3\"]\n",
    "print(doc_names)\n",
    "print(list(flatten(similarity_docs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
